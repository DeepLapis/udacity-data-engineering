# Data Modeling with Postgres

In this project, a (hypothetical) startup Sparkify, would want to analyze data they have collected on their music app. They would like to understand the songs that their users are listening to. However, the data is difficult to query because they are JSON logs. 

Here is where the data engineer would create a Postgres database with tables that makes data easy to retrieve and analyze. A database schema and ETL pipeline would be created and tested by using queries from the analytics team.

## Directory Structure

```
Data Modeling with Postgres
|____data			    # Datasets
| |___log_data            # Session related data
| |___song_data           # Song metadata
|
|___notebook			# Notebook for quick testing
| |___etl.ipynb		      # ETL logic are developed here
| |___test.ipynb		  # To observe if data is inserted correctly	
|
|___src			        # Source code
| |___etl.py			  # Performs ETL 
| |___sql_queries.py      # Query helper functions
| |___create_tables.py    # Database and tables creation script
```



### Datasets 

The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Set up (No Docker)
### Setting up Postgres locally
Using Terminal (for macOS), ensure that postgresql is installed through brew by running
``` brew install postgresql ```

When completed, run:

``` brew services start postgresql ``` 

This starts postgres and we can proceed to create a user and password

Run:

```psql postgres ``` to start and ```\q``` to quit

### Creating the student user
The setup specified by Udacity are as follows

1. Two databases: studentdb and sparkifydb
2. User = student and password = student (obviously you do not share these credentials in a real project. Store them in a .env file or a file ignored by git through .gitignore)

Check that available users and databases by running: ```\l```. If you are just starting out, it is likely that ```studentdb``` and ```sparkifydb``` under the ```student``` user would not exist and hence you should create them with this line:

``` CREATE USER student WITH PASSWORD 'student' CREATEDB;```

Then, proceed to create the database by running

```CREATE DATABASE studentdb OWNER student```

and

```CREATE DATABASE sparkifydb OWNER student```

Once done you can run ```\q``` and it's time to run the scripts!

### Other Dependencies

```
jupyter-lab
SQLAlchemy==1.4.29
postgres==4.0
psycopg2-binary==2.9.3
psycopg2-pool==1.1
pandas==1.3.4
```
The project was running on a master virtual environment in my laptop hence the lack of a ```requirement.txt``` file. I've managed to isolate the more important packages that can be installed to run the scripts.

### Program Execution
Run ```python3 src/create-table.py``` to establish the connection to the Postgres database and created the relevant tables. 

Then running ```python3 src/etl.py``` would perform a series of steps as described below:

#### Extract
* Data from the JSON regarding the song's metadata and the user activity log are extracted and stored in a pandas dataframe for easier analysis and data transformation

#### Transform
* As the timestamp is stored in milisecond, pandas' ```datetime``` method was used to convert the the timestamps to format easier to work with such as listing days, week, months, hour, day of week etc

#### Load
* When the data is extracted and transformed, they are loading accordingly into the Postgres database for the analyst to run their queries

When the rows are loaded properly, you will be able to access the relevent tables seen below through PostgreSQL

### Schema

#### Fact Table
```
songplays
	songplay_id SERIAL PRIMARY KEY ,
	start_time BIGINT,
    user_id INT,
    level VARCHAR,
    song_id VARCHAR,
    artist_id VARCHAR,
    session_id INT,
    location VARCHAR,
    user_agent VARCHAR

```

#### Dimension Tables
```
users
	user_id INT PRIMARY KEY,
    first_name VARCHAR,
    last_name VARCHAR, 
    gender VARCHAR,
    level VARCHAR

songs
	song_id VARCHAR PRIMARY KEY,
    title VARCHAR,
    artist_id VARCHAR,
    year INT,
    duration FLOAT

artists
	artist_id VARCHAR,
    name VARCHAR,
    location VARCHAR,
    latitude FLOAT,
    longitude FLOAT

time
	start_time BIGINT PRIMARY KEY,
    hour INT,
    day INT,
    week INT,
    month INT,
    year INT,
    weekday INT
```

## Tear Down
When you are done with everything run

```brew services stop postgresql```